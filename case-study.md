# Case-study оптимизации

## Актуальная проблема

1) Добавила в проект тесты
2) Начем с оптимизации импорта данных

## Оптимизация импорта данных

напишем раннер для запуска / мониторинга времени / памяти выполнения скрипта импорта

1) время выполнения fixtures/small.json - 10.167809
2) воспользуемся rbspy для профилирования скрипта импорта
3) rbspy / rubyprof выдают много лишних данных при вызове таски, поэтому можно вынести импорт в отдельный класс и профилировать его
4) вынесла в класс DataLoader, перенесла тесты

Пока выносила обратила внимание что файл считывается в память целиком, также в задании упоминается про файл 1М который весит примерно 3ГБ, перепишем сразу на стриминг
добавила класс jsonStreamer - собирает объекты и возвращает по одному, какой то адекватный гем не нашла для такой обработки файла, теперь используется потоковая обработка файла


1) время выполнения не поменялось
2) время выполнения fixtures/small.json - 10.473295
3) отчет stackfrof - главная точка роста:
в отчете видны 2 главные точки роста - update (41%) и find_or_create_by (25%)
4) что делать с update пока непонятно, а вот такое кол-во find_or_create_by можно заменить
5) Внесем правки в работу с городами, написано что в файле их не больше 100, попробуем собирать их в словать параллельно создавая
6) время выполнения fixtures/small.json - 8.523640
7) изменения в отчете профилировщика: find_or_create_by снизился до 18%

1) также у нас собирается еще 2 стравочника - services, buses
2) заменим find_or_create_by и там
3) services: 6.919434, find_or_create_by 23%
4) buses: 5.319638, find_or_create_by больше нет

1) главная точка роста: update (64%)
2) обновляется автобус сервисами, это обновление в целом выглядит довольно бесполезно, потому что сервисы останутся от последнего trip в файле
3) поэтому обновление сервисами можно в принципе убрать, либо уточнить формат файла, должны ли они добавляться / обновляться или браться пересечение
4) убрали обновление заменив его на создание сервисов при создании автобуса
5) время: 3.025756
6) отчет профилировщика: update больше нет

1) главная точка роста: создание записей
2) обратимся к логам и посмотрим на кол-во обращений к базе
3) на файл example к базе было 10 (создание trip) + 2 (проверка существования автобуса + создание) + 4 (проверка городов + создание) + 2 (создание сревисов) = 18
4) можно воспользоваться стримингом из readme
5) время для small: 0.138599
6) medium: 0.510029
7) large: 3.958625
8) 1M: 38.381950
9) ну тут уже время упирается в стример, без каких либо преобразований он перебирает файл 1M за 34.973238
10) причем потребление памяти на 1М не превышает 8МБ
```
INITIAL MEMORY USAGE: 103 MB
MEMORY USAGE: 103 MB
MEMORY USAGE: 110 MB
... 7 строчек с 110 MB
MEMORY USAGE: 110 MB
MEMORY USAGE: 111 MB
... 26 строчек с 111 MB
MEMORY USAGE: 111 MB
FINAL MEMORY USAGE: 110 MB
```


